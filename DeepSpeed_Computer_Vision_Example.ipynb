{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c6eae57",
   "metadata": {},
   "source": [
    "### Computer Vision using DeepSpeed and MPI\n",
    "\n",
    "In this advanced tutorial, you will learn more about ``deepspeed`` and the Zero Redundancy algorithms (ZeRO).  These algorithms are part of the magic of ``deepspeed`` which enables training of billion plus parameter models on small clusters of GPUs.  Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57e34e",
   "metadata": {},
   "source": [
    "#### What the learner will be able to do at the end of this tutorial:\n",
    "\n",
    "- Execute a simple computer vision pipeline\n",
    "- Explain how to modify code to run in the DeepSpeed framework\n",
    "- Understand the subtle difference between running within the DeepSpeed framework versus using MPI to run DeepSpeed pipelines\n",
    "- Assess whether a pipeline would be more efficiently run in python or a DeepSpeed framework\n",
    "- Contrast ZeRO optimizers in experiments by changing the configuration file\n",
    "\n",
    "#### What is DeepSpeed?\n",
    "\n",
    "DeepSpeed, was originally released in early 2020 as an open-source software from Microsoft Research. DeepSpeed is a framework for training and doing inference with very large parameter, high memory usage models.  It is built to run on MPI which allows DeepSpeed to manage resources fluidly.  See the beginner's MPI notebook for information on MPI works and shares memory. The resources DeepSpeed manages include shared memory available on GPUs, CPUs and Non-Volatile Memory express or NVMe for short.  As a gentle reminder GPU hardware also contains a limited number of CPUs to run some basic operations. \n",
    "\n",
    "#### What are ZeRO optimization algorithms?\n",
    "\n",
    "The ZeRO algorithms contained in DeepSpeed take advantage of all the hardware memory on GPUs. Scaling with DeepSpeed and its ZeRO optimization algorithms enables scaling models from those that can run on a single GPU to sizes that run on High Performance Clusters of GPUs.  Models trained can be over a billion parameters wide. Open-sourced models tested and trained with DeepSpeed include Megatron, GPT-2, Yanex's YaML-100B.\n",
    "\n",
    "#### How do ZeRO optimization algorithms help with training large parameter models or models with large datasets?\n",
    "\n",
    "As we mentioned, DeepSpeed takes advantage of the ZeRO (Zero Redundancy Optimizer) algorithms.  These include:\n",
    "\n",
    "- ***ZeRO (stage 1)*** - an algorithm in which an Adam optimizer, 32-bit weights, the first, and second moment estimates are partitioned across the processes, so that each process updates only its partition.\n",
    "\n",
    "- ***ZeRO2 (stage 2)*** - an algorithm which uses reduced 32-bit gradients for updating the model weights.  Weights are also partitioned such that each process retains only the gradients corresponding to its portion of the optimizer states.\n",
    "\n",
    "- ***ZeRO3 (stage 3)*** - an algorithm which partitions a 16-bit model's parameters across the processes. ZeRO-3 will automatically collect and partition parameters and weights during the forward and backward passes.\n",
    "\n",
    "There are many other components of DeepSpeed and the library is evolving its technologies.  Check out the references at the end of the tutorial and Microsoft's blogs on DeepSpeed.\n",
    "\n",
    "For more detailed information see Microsoft's blog article: [Zero Redundancy Optimizers](https://www.deepspeed.ai/tutorials/zero/)\n",
    "\n",
    "#### Getting Started on this Tutorial\n",
    "\n",
    "In this tutorial we will look at training a computer vision pipeline with DeepSpeed.  We will run the code in more than one way, demonstrating that the command ``deepspeed`` can be used, or the code can be run with ``mpirun`` command.  The advantage of ``mpirun`` is more granular control of the run.  For example one can set the number of processes, ``map-by``to slot number, and ``bind-to`` configurations using ``mpirun``.  Let's dive right in!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1597f500",
   "metadata": {},
   "source": [
    "As a best practice, lets start off by checking that the host files are correctly listed when the hostname command is called.  This indicates to us that the cluster is set-up to run correctly.  If the hostnames do not show up, it is best to restart the kernal and then *sync* the MPI workers again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af27c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf0ac2",
   "metadata": {},
   "source": [
    "Now just for comparison's sake below is the method to run the cifar example code without MPI or DeepSpeed.  Do not run this in class as it will take a long time to finish.  You can run this after the workshop on your own to make a comparison between methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313b2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# run this at home or when time is not an issue -- it will take much longer than running with MPI or deepspeed\n",
    "\n",
    "#!python /mnt/data/DeepSpeed_Repository_Code/DeepSpeedExamples-master/cifar/cifar10_tutorial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36235fec",
   "metadata": {},
   "source": [
    "#### Running our Pipeline on MPI without DeepSpeed\n",
    "\n",
    "Now let's look to see what happens when we use the same ``mpirun`` command as before without the use of DeepSpeed.  How long does it take to run?  Why do you think it takes that amount of time?  Hint: when we run code with ``mpirun`` alone, we're not using memory optimization techniques.  That means that there is more information / data in cache and it has not been offloaded to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2478a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!mpirun -np 2 python cifar10_tutorial.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6922682f",
   "metadata": {},
   "source": [
    "#### Using Code Initialized for DeepSpeed\n",
    "\n",
    "Now let's experiment with ``deepspeed``.  Take a look at the file called ``cifar10_deepspeed.py``.  Notice that the following code has been added to the file.  Looking as the ``deepspeed`` repository we discover that to initialize ``deepspeed`` on a cluster we need to initialize with a ``deepspeed`` command instead of a ``torch_distributed`` command.\n",
    "\n",
    "Take a look at this code snippet.\n",
    "\n",
    "```\n",
    "# Initialize DeepSpeed to use the following features\n",
    "# 1) Distributed model\n",
    "# 2) Distributed data loader\n",
    "# 3) DeepSpeed optimizer\n",
    "model_engine, optimizer, trainloader, __ = deepspeed.initialize(\n",
    "    args=args, model=net, model_parameters=parameters, training_data=trainset)\n",
    "\n",
    "fp16 = model_engine.fp16_enabled()\n",
    "print(f'fp16={fp16}')\n",
    "```\n",
    "In the examples below we try the following and see how the time compares:\n",
    "\n",
    "- Training and inference using the ``deepspeed`` command and a zero optimizer.\n",
    "\n",
    "- Training and inference using the ``deepspeed`` command and a zero2 optimizer.\n",
    "\n",
    "- Training and inference using the ``mpirun`` command and a zero optimizer.\n",
    "\n",
    "What do you think the difference is between using the ``deepspeed`` command and the ``mpirun`` command?  Hint: DeepSpeed is a framework built on top of MPI.  Do you think its quicker to use MPI without casting the ``deepspeed`` framework?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ae0a9",
   "metadata": {},
   "source": [
    "#### What does it take to run distributed compute using DeepSpeed?\n",
    "\n",
    "There a couple of minor code changes required when using pytorch in order to run in the deepspeed framework or on an MPI / deepspeed pipeline.  \n",
    "    \n",
    "Here is a line of code required to run on ``deepspeed``:\n",
    "\n",
    "```\n",
    "model_engine, optimizer, trainloader, __ = deepspeed.initialize(\n",
    "    args=args, model=net, model_parameters=parameters, training_data=trainset)\n",
    "\n",
    "fp16 = model_engine.fp16_enabled()\n",
    "print(f'fp16={fp16}')\n",
    "\n",
    "```\n",
    "Of particular note is the function ```deepspeed.initialize()``` which tells the code to run the ``deepspeed`` framework.  It ***does not*** indicate which optimizer to use.  That information is present in the configuration file.\n",
    "\n",
    "There may be other modifications required to your file but its best to start very simply with the modifications above which we used for this computer vision example.  If you have more questions or would like specialized help with your code, please reach out to the FDS team at Domino Data Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1bb08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## running without a host file, with the Zero optimizer\n",
    "\n",
    "!deepspeed cifar10_deepspeed.py --deepspeed --deepspeed_config ds_config_simple.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e102a54",
   "metadata": {},
   "source": [
    "Now let's take a look at what happens when we use the hostfiles.  Remember in order to recognize the GPUs / cluster we need to give ``deepspeed`` or ``mpirun`` the hostfile names and addresses.  How does the time change?  How does the output change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ad7670",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## run with the mpi hostfiles and stage zero optimization, in this example the pipeline will run on every mpi 'host'\n",
    "\n",
    "!deepspeed --hostfile /domino/mpi/hosts cifar10_deepspeed.py --deepspeed --deepspeed_config ds_config2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae58a8",
   "metadata": {},
   "source": [
    "#### Running a DeepSpeed file using MPI\n",
    "\n",
    "As long as ***mpi4py*** is installed along with ***deepspeed***, the ``mpirun`` command can be used, just like we did for the calculation of pi examples.  Try running the code below 'as is' and then try adding a few of the variables.  Here are some things to think about.\n",
    "\n",
    "- Change the number of processes.  How does this affect the wall clock time?  Do fewer processes speed up the code?  Hint, think about parallelism\n",
    "\n",
    "- Try using the ``mpirun`` commands ``map-by`` and ``bind-to``.  What affect, if any, does this have on how the code runs?\n",
    "\n",
    "- Why do you think OpenMPI command ``mpirun`` makes DeepSpeed run faster than using ``deepspeed`` command only?\n",
    "\n",
    "- What do you think happens if we used DeepSpeed ZeRO2 memory optimizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f58447c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!mpirun -np 2 --hostfile /domino/mpi/hosts python cifar10_deepspeed.py \\\n",
    "--deepspeed_config ds_config_zero2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cdcae1",
   "metadata": {},
   "source": [
    "#### Changing the DeepSpeed Zero Optimizer\n",
    "\n",
    "Now let's take this simple example and see what happens when we use the Zero optimizer 2 instead of 1 (as in the original configuration file).  Does the code run any differently?  Do you think this would be a more impactful difference if it were a bigger model with more data?\n",
    "\n",
    "Code changes:\n",
    "\n",
    "```\n",
    "    {\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"contiguous_gradients\": true,\n",
    "        \"overlap_comm\": true,\n",
    "        \"reduce_scatter\": true,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"allgather_bucket_size\": 5e8}\n",
    "    }\n",
    "```\n",
    "\n",
    "Look at the json file called ds_config_zero2.json.  See if you can find the code change.  Now try running the mpi code again with the ZeRO3 optimizer. Hint: Why does it take slightly longer to run when more memory is offloaded?  Under what circumstance might this be helpful?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfa78f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!mpirun -np 2 --hostfile /domino/mpi/hosts python cifar10_deepspeed.py \\\n",
    "--deepspeed_config ds_config_zero2.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ff057f",
   "metadata": {},
   "source": [
    "#### What we learned in this tutorial\n",
    "\n",
    "- DeepSpeed runs on MPI (under the hood).\n",
    "\n",
    "- MPI can be used to run DeepSpeed files or one can choose the DeepSpeed framework.\n",
    "\n",
    "- Optimizing memory is as simple as changing one or two lines of a configuration file.\n",
    "\n",
    "- Code changes are required to a ``pytorch`` file in order to run in ``deepspeed`` in distributed mode are simple.\n",
    "\n",
    "- DeepSpeed is appropriate for large parameter models because it handles memory well.\n",
    "\n",
    "- Deepspeed / MPI clusters do not have a user-interface, but the output is verbose.\n",
    "\n",
    "\n",
    "#### More resources on DeepSpeed and MPI\n",
    "\n",
    "There are some excellent references and tutorials on DeepSpeed.  Try some of the examples on your own or try a large model like GTP-2 and see how it performs.  DeepSpeed will enable the use of less hardware and greater memory efficiency.  However unlike the name, for smaller models or less data, DeepSpeed may not necessarily speed up code runtime.  DeepSpeed however is extremely powerful for training very large models in particular large language or transformer models like GPT-2, Megatron or Yanex's YaML-100B billion parameter model.  Read some of the references below to learn more.\n",
    "    \n",
    "- [Basics of MPI](https://www.markdownguide.org/basic-syntax/)\n",
    "\n",
    "- [OpenMPI Documentation](https://www.open-mpi.org/faq/?category=running#simple-launch)\n",
    "\n",
    "- [DeepSpeed Documentation](https://www.deepspeed.ai/)\n",
    "\n",
    "- [DeepSpeed Repository with more Practice Exercises](https://github.com/microsoft/DeepSpeedExamples)\n",
    "\n",
    "- [Training a very large model with hugging face -- try some of these at home!](https://huggingface.co/docs/transformers/main_classes/deepspeed)\n",
    "\n",
    "- [Train Megatron using DeepSpeed](https://www.deepspeed.ai/tutorials/megatron/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
