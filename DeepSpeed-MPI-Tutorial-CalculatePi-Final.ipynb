{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d23dbdf",
   "metadata": {},
   "source": [
    "### Absolute Beginners Tutorial for MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07493aee",
   "metadata": {},
   "source": [
    "Today we'll review an absolute beginner's use of MPI and DeepSpeed.  For this tutorial we'll use OpenMPI which is a message passing interface often used at super-computing centers for high performance computing.  It can be used for problems that require parallelism or those that require distributed computing (where processes require careful coordination).  OpenMPI on Domino Data Lab is easy to use.  \n",
    "\n",
    "Here are a few tips on hardware choices:\n",
    "\n",
    "1. Make sure to choose the correct hardware.  If working with small to medium size data (under a gigabyte) often a small or medium hardware tier is sufficient.\n",
    "2. If using large datasets (larger than a GB) it is often useful to select a high memory hardware tier in either a CPU or GPU.\n",
    "3. If using a complex calculation with very large data (larger than a few GB) then hardware \n",
    "\n",
    "For DeepSpeed use cases, the models and / or data is typically large, so a high-memory, GPU is most helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a7d7ed",
   "metadata": {},
   "source": [
    "### What is OpenMPI?\n",
    "\n",
    "A message passing interface (MPI) is a computer program that passes communications between hardware on a cluster and helps to manage shared memory.\n",
    "\n",
    "The Open MPI Project is an open-sourced MPI implementatation that is created and maintained by a group of academic, research and industry partners.  Its a popular choice for Higher Performance omputing and supercomputing centers. \n",
    "\n",
    "Some of the features of OpenMPI include:\n",
    "\n",
    "- Conforms to MPI-3.1 standards\n",
    "- Thread management and concurrency (your computer will not get 'fried')\n",
    "- Support of all networks\n",
    "- Supports most job schedulers\n",
    "- High Performance on all platforms\n",
    "- Open source license (BSD license)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75f5122",
   "metadata": {},
   "source": [
    "### How MPI is used in High Performance Computing\n",
    "\n",
    "High Performance Computing (HPC) is simply -- at high speeds -- performing compex calculations.  It run both parallel and distributed problems.  The best known example of HPC is a supercomputer.  Supercomputers are made up of many CPUs or GPUs and processors working together to perform both parallel and distributed tasks.  MPI aids in this by coorindating messaging among CPUs, GPUs and nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23a03e7",
   "metadata": {},
   "source": [
    "### What the learner will be able to do at the end of this tutorial:\n",
    "\n",
    "Upon completion of part one of this tutorial the learner will be able to:\n",
    "    \n",
    "- Describe OpenMPI and its use cases.\n",
    "- Identify whether code is running with parallelism or distributed across a cluster.\n",
    "- Perform a distributed calculation of pi using MPI workers.\n",
    "- Explain the use of python files to run programs on a MPI-managed cluster.\n",
    "- Determine what commands are necessary in mpi4py to distribute calculations across a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f74d8bd",
   "metadata": {},
   "source": [
    "#### A 'Hello World' Example\n",
    "\n",
    "In order to run a program or python files using MPI and python the code in the file needs to be set-up to run on the cluster.  In this exercise we will look at the number of processes running and the rank for each.  This ``hello world`` example is run from a sepparate python file.  One can run many python programs in the same manner.\n",
    "\n",
    "Note: make sure to sync your MPI cluster before running new code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee99bb0e",
   "metadata": {},
   "source": [
    "```\n",
    "from __future__ import print_function\n",
    "from mpi4py import MPI\n",
    "comm = MPI.COMM_WORLD\n",
    "print(\"Hello! I'm rank %d from %d running in total...\" % (comm.rank, comm.size))\n",
    "comm.Barrier()\n",
    "\n",
    "```\n",
    "\n",
    "Notice in the code above we set up the communicator, which indicates the 'world' size or number of cpus or gpus in the cluster on which the program will run.  The ```comm.Barrier()``` function tells the program to wait until all processes and workers are synced.  Once the code is properly formed the program can be run at the command line using ``mpirun`` along with the appropriate options.  Try this for yourself changing the number of processes.  You can also add or change the 'hello world' code to run a different python function.  What happens when you change the number of processes (``-np``)?  Does it go faster, slower or the same?  How does the print-out change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861d4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316530a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpirun -np 1  python hello_world.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1e85f5",
   "metadata": {},
   "source": [
    "Once can also use the command ``mpiexec`` as an execution command for a python file.  There is very little difference between ``mpirun`` and ``mpiexec``.  Feeding the hostfile location to the mpi run will make explicit to the mpi run, the location of each worker.  This facilitates the distribution of calculations or runs across workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a46d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mpiexec --hostfile /domino/mpi/hosts -np 1 python hello_world.py "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a5e898",
   "metadata": {},
   "source": [
    "#### Calculating Pi on Worker Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d375f793",
   "metadata": {},
   "source": [
    "Below is the code we use to run a generic python file which calculates pi.  You may notice that there is little difference in the time to run or the output of the cpi2.py file.  This is in part because we have not broadcasted the data to the workers, thus we are not running the file in a distributed manner.  Rather the file is just running once on each worker process indicated.  Keep reading, we will look at how to distribute a calculation over the mpi workers with the python library mpi4py.  Try experimenting with the number of processes used (np) in the mpirun.  You'll see that each change in process number simply runs it on that number of workers, but in a parallel manner, not distributed.\n",
    "\n",
    "The code in the cpi2.py file is as such:\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "N=10**8\n",
    "\n",
    "h = 1.0 / N; s = 0.0\n",
    "for i in range(N):\n",
    "    x = h * (i + 0.5)\n",
    "    s += 4.0 / (1.0 + x**2)\n",
    "    estimated_pi = s * h\n",
    "\n",
    "print(estimated_pi)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e211465",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!python cpi2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fe934",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "!mpirun -np 1 --hostfile /domino/mpi/hosts --bind-to none --map-by slot python cpi2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03eed2c0",
   "metadata": {},
   "source": [
    "Notice that the code when run using ``mpirun`` was not significantly faster than running as a vanilla python file.  Why do you think this is?  Hint: there's a difference between simple parallelism and distributed functions. Try changing the number of processes again.  You'll notice the caluclation of pi prints out the same number of times as the number of processes.  Why do you think that is?\n",
    "\n",
    "The MPI program also has an easy command to call a list of options for ``mpirun`` and other mpi commands.  See the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930b14a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### how to reach the mpi help file\n",
    "\n",
    "!mpirun --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2faf1c2",
   "metadata": {},
   "source": [
    "#### Calculating Pi with and without Distributed Compute\n",
    "\n",
    "Below let's look at an example of how to calculate pi without mpi (naïve method) and with mpi (distributed method).  The formula we'll use to calculate pi is simple. The number pi is a ratio obtained from defining the area with a circle. If the diameter and the circumference of a circle are known, the value of pi will be as π = circumference of a circle/diameter of a circle.  In our calculation we take a number of samples ``N`` to estimate pi more accurately and to test whether distributing these calculation over workers is more efficient.  This is basically the Leibniz formula which you can find out more about it [here](https://en.wikipedia.org/wiki/Leibniz_formula_for_%CF%80).\n",
    "\n",
    "A more complex example of calculating pi uses a monte carlo calculation and the code is available in the supplemental materials.  That example runs on ``mpi`` using the python library, ``mpi4py``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d67fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "N=10**8\n",
    "\n",
    "h = 1.0 / N; s = 0.0\n",
    "for i in range(N):\n",
    "    x = h * (i + 0.5)\n",
    "    s += 4.0 / (1.0 + x**2)\n",
    "    estimated_pi = s * h\n",
    "\n",
    "print(estimated_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a67a48",
   "metadata": {},
   "source": [
    "We can also run this code in a python file using ``mpirun``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacc07ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "!mpirun python /mnt/cpi2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc7fb00",
   "metadata": {},
   "source": [
    "Now let's compare that calculation to calculating pi using OpenMPI.  For this function we will use a python wrapper / library around MPI called ``mpi4py``.  We will keep our calculation code in a sepparate file so we can run the function over MPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396ac033",
   "metadata": {},
   "source": [
    "#### Client Side Code\n",
    "\n",
    "The client-side code will output our calcuation of pi from the file cpi.py.  We will take a look at the client side code that is in the cpi.py file after we run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "## prepare for distributed calculation using MPI\n",
    "\n",
    "maxprocs = input('How many GPU workers do you have?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7190e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "import sys\n",
    "import mpi4py\n",
    "\n",
    "\n",
    "\n",
    "comm = MPI.COMM_SELF.Spawn(sys.executable,\n",
    "                           args=['cpi.py'],\n",
    "                           maxprocs=int(maxprocs))\n",
    "\n",
    "N = numpy.array(10**8, 'i')\n",
    "comm.Bcast([N, MPI.INT], root=MPI.ROOT)\n",
    "PI = numpy.array(0.0, 'd')\n",
    "comm.Reduce(None, [PI, MPI.DOUBLE],\n",
    "            op=MPI.SUM, root=MPI.ROOT)\n",
    "print(PI)\n",
    "\n",
    "comm.Disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33c869",
   "metadata": {},
   "source": [
    "The code above uses ``Spawn`` to initiate the communications across a cluster, the ``Bcast`` to broadcast the calculation and numpy array across workers and use the function ``Reduce`` to reduce the calculations across workers to a final answer that will be printed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cac2ec",
   "metadata": {},
   "source": [
    "#### Server Side Code (contained in cpi.py)\n",
    "\n",
    "Notice the code below contains our generic calculation of pi using the Lebniz model. Consider using the paradigm with mpi4py in which the calculations will be truly distributed via broadcating to the workers and reducing the worker's calculations to a final answer.  Optionally you can put both server-side and client side code into one file.  Use the instructions earlier to run the file on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30b8cd5",
   "metadata": {},
   "source": [
    "The server-side code for the MPI example above looks like this:\n",
    "\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "from mpi4py import MPI\n",
    "import numpy\n",
    "\n",
    "comm = MPI.Comm.Get_parent()\n",
    "size = comm.Get_size()\n",
    "rank = comm.Get_rank()\n",
    "name = MPI.Get_processor_name()\n",
    "\n",
    "N = numpy.array(0, dtype='i')\n",
    "comm.Bcast([N, MPI.INT], root=0)\n",
    "h = 1.0 / N; s = 0.0\n",
    "for i in range(rank, N, size):\n",
    "    x = h * (i + 0.5)\n",
    "    s += 4.0 / (1.0 + x**2)\n",
    "PI = numpy.array(s * h, dtype='d')\n",
    "comm.Reduce([PI, MPI.DOUBLE], None,\n",
    "            op=MPI.SUM, root=0)\n",
    "\n",
    "comm.Disconnect()\n",
    "\n",
    "```\n",
    "We see the instantiation of the following variables:\n",
    "\n",
    "- comm - the parent communication worker (usually worker 0)\n",
    "- size - the size of the cluster\n",
    "- rank - the rank of each GPU worker in the cluster (example if there are five the ranks are 0, 1, 2, 3, 4)\n",
    "- name - the processor name\n",
    "\n",
    "These variables are required so that messages can be passed between workers / shared memory and a reduction operation can present the end results of our calculation of pi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528a1c2",
   "metadata": {},
   "source": [
    "#### What we learned in this tutorial:\n",
    "\n",
    "- OpenMPI stands for the open-source version of the message passing interface software.\n",
    "\n",
    "- MPI can be used to run generic python files using ``mpirun`` or ``mpiexec``.\n",
    "\n",
    "- Care must be taken to run code in a distributed manner on a cluster rather than running solely in parallel.\n",
    "\n",
    "- Code changes are required with the use of the ``mpi4py`` library in order to distribute data or calculations.\n",
    "\n",
    "- Choosing the correct hardware will make sure calculations run smoothly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca68516",
   "metadata": {},
   "source": [
    "#### To learn more see these references and tutorials:\n",
    "    \n",
    "[Basics of MPI](https://carleton.ca/rcs/rcdc/introduction-to-mpi)\n",
    "\n",
    "[OpenMPI Documentation](https://www.open-mpi.org/faq/?category=running#simple-launch)\n",
    "\n",
    "[Parallel Programming with MPI and Python](https://rabernat.github.io/research_computing/parallel-programming-with-mpi-for-python.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
